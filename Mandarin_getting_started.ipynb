{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "719050b6",
      "metadata": {
        "id": "719050b6"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e11db7",
      "metadata": {
        "id": "61e11db7"
      },
      "source": [
        "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
        "\n",
        "# The Making of the Riva Mandarin ASR Service\n",
        "\n",
        "This notebook walks you through the end-to-end process that NVIDIA engineers and data scientists employed to develop the Riva Mandarin Automatic Speech Recognition (ASR) service, from raw transcribed audio data to a ready-to-serve Riva ASR service.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The below diagram provides a high-level overview of the end-to-end engineering workflow required to realize the Riva Mandarin ASR service.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='Mandarin-Riva.PNG')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "hgdhFyMlPSxy",
        "outputId": "d1d31fb0-8c5c-4be5-c4f4-cfd76ad66866"
      },
      "id": "hgdhFyMlPSxy",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-643bfaf586ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Mandarin-Riva.PNG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Mandarin-Riva.PNG'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3417f79c-27d9-40b6-8929-eca53f73245c",
      "metadata": {
        "id": "3417f79c-27d9-40b6-8929-eca53f73245c"
      },
      "source": [
        "Beyond the data collection phase, the new language workflow for Riva is divided into three major stages:\n",
        "- Data preparation\n",
        "- Training and validation\n",
        "- Riva deployment\n",
        "\n",
        "In the next sections, we look deeper into each of these stages."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a237f9-2dbb-45b8-b8d9-2bca53101c4d",
      "metadata": {
        "id": "a0a237f9-2dbb-45b8-b8d9-2bca53101c4d"
      },
      "source": [
        "## 1. Data collection\n",
        "When adapting Riva to a whole new language, a large amount of high-quality transcribed audio data is critical for training high-quality acoustic models. \n",
        "\n",
        "There are several Mandarin speech corpus that we obtained from open-source and vendors: \n",
        "\n",
        "Open-sourced corpus: \n",
        "- [AIShell-1](https://arxiv.org/pdf/1709.05522.pdf) 178 hours, for conducting the speech recognition research and building speech recognition systems for Mandarin.\n",
        "- [AIShell-2](https://arxiv.org/pdf/1808.10583.pdf) 1000 hours clean read-speech data from iOS.\n",
        "\n",
        "Proprietary datasets: \n",
        "- 4 speech datasets: 1018 hours.\n",
        "- 3 speech datasets: 510 hours.\n",
        "\n",
        "In total, we have 2707 hours of Mandarin speech audio data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5af4eb6c-97c7-454f-87cf-f5bc97328c96",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "5af4eb6c-97c7-454f-87cf-f5bc97328c96"
      },
      "source": [
        "## 2. Data preparation\n",
        "\n",
        "The data preparation phase carries out a series of preparation steps required to convert the diverse raw audio datasets into a uniform format that can be efficiently digested by NVIDIA NeMo toolkit. These steps are:\n",
        "\n",
        "### 2.1. Data preprocessing\n",
        "\n",
        "**Audio data**: Audio data acquired from various sources are inherently heterogeneous (file format, sample rate, bit depth, number of audio channels...). Therefore, as a preprocessing step, we build a separate data ingestion pipeline for each source and convert these audio data to a common format with the following characteristics:\n",
        "- Wav format\n",
        "- Bit depth: 16 bits\n",
        "- Sample rate of 16 Khz\n",
        "- Single audio channel\n",
        "\n",
        "**Text data**: \n",
        "\n",
        "- Dataset ingestion scripts are used to convert the various datasets into the standard manifest format expected by NeMo, you can refer to [NeMo data processing scripts](https://github.com/NVIDIA/NeMo/tree/main/scripts/dataset_processing).\n",
        "\n",
        "- Text Normalization converts text from written form into its verbalized form. It is used as a preprocessing step for preprocessing Automatic Speech Recognition (ASR) training transcripts.\n",
        "\n",
        "### 2.2. Data cleaning/filtering\n",
        "\n",
        "This step is carried out to filter out some outlying samples in the datasets. \n",
        "\n",
        "- Samples that are too long, too short or empty are filtered out.\n",
        "\n",
        "- Samples with high CER (character error rate): Use [Nemo data explorer](https://github.com/NVIDIA/NeMo/tree/main/tools/speech_data_explorer) to find proper CER for each dataset. Then filter out samples having very high CER.\n",
        "\n",
        "- Keep samples which have durations in [0.1, 20] seconds\n",
        "\n",
        "\n",
        "### 2.3. Train and Test splitting\n",
        "This step is a staple of the any deep learning and machine learning development pipeline, to ensure that the model is learning to generalize without overfitting the training data. For the dev set, we use 1% of the training datasets for validation. For the test set, we additionally curated data that isn't from the same source as the training datasets, such as Youtube."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b800ff3-a9ff-49e8-aa15-e305f377f244",
      "metadata": {
        "tags": [],
        "id": "8b800ff3-a9ff-49e8-aa15-e305f377f244"
      },
      "source": [
        "## 3. Training and validation\n",
        "\n",
        "The models in an ASR pipeline include:\n",
        "\n",
        "- An **acoustic model**, that maps raw audio input to probabilities over text tokens at each time step.  This matrix of probabilities is fed into a decoder, that convert probabilities into a sequence of text tokens.\n",
        "- A **language model**, that is optionally used in the decoding phase of the acoustic model output. \n",
        "- A **punctuation model**, that formats the raw transcript, augmenting with punctuation.\n",
        "\n",
        "\n",
        "### 3.1. Acoustic model\n",
        "\n",
        "The acoustic model is by far the most important part of an ASR service. These are the most resource intensive models, requiring a large amount of data to train on powerful GPU servers or cluster. They also have the largest impact on the overall ASR quality.\n",
        "\n",
        "**Model architecture**: [Citrinet](https://arxiv.org/pdf/2104.01721.pdf) is a new end-to-end convolutional Connectionist Temporal Classification (CTC) based ASR model. Citrinet is deep residual neural model which uses 1D time-channel separable convolutions combined with sub-word encoding and squeeze-and-excitation.\n",
        "The resulting architecture significantly reduces the gap between non-autoregressive and sequence-to-sequence and transducer models.\n",
        "\n",
        "The model we experimented for the Mandarin ASR pipeline is Citrinet-1024. The final model chosen for deployment of the Riva Mandarin ASR service (ver. 22.04) was finetuned from [STT Zh Citrinet 1024 Gamma 0.25](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_zh_citrinet_1024_gamma_0_25). \n",
        "\n",
        "**Training script**: We leverage NeMo training [scripts](https://github.com/NVIDIA/NeMo/blob/v1.0.2/examples/asr/speech_to_text.py). \n",
        "\n",
        "**Hyper-parameter setting**: For model training, we used: 500 epochs, batch size 32, learning rate 0.005, beta parameters [0.8, 0.25], weight decay 0.001.\n",
        "\n",
        "**Training environment**: We trained the Citrinet model on a GPU cluster comprising 64 GPUs, each taking 182 hours."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c980e9a1-4e36-40b1-8951-143669190fcf",
      "metadata": {
        "id": "c980e9a1-4e36-40b1-8951-143669190fcf"
      },
      "source": [
        "### 3.2. Language model\n",
        "\n",
        "Language model, combined with beam search in the decoding phase can further improve the quality of the ASR pipeline. In our experiments, we generally observe WER reduction by using a simple n-gram model. \n",
        "\n",
        "The language models supported by Riva are n-gram model, which can be trained with the Kenlm toolkit. See Riva [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-customizing.html#training-language-models) for details on how to train and deploy a custom language model.\n",
        "\n",
        "**Training data**: We create training set by combining all the transcript text in our ASR set then normalizing them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54f7ef88-8089-42b9-b848-b1a15301ce6f",
      "metadata": {
        "id": "54f7ef88-8089-42b9-b848-b1a15301ce6f"
      },
      "source": [
        "### 3.3. Punctuation model\n",
        "\n",
        "The Punctuation model consists of the pre-trained Bidirectional Encoder Representations from Transformers (BERT).\n",
        "The model was trained with BERT base multilingual cased checkpoint on the ASR transcription and a subset of data from the [Tatoeba](https://tatoeba.org/en/) website's Chinese sentences.\n",
        "\n",
        "We employed a BERT-base model for the task and leverage NeMo [script](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/token_classification/punctuation_capitalization_train_evaluate.py) for the training part. \n",
        "\n",
        "See also NeMo [tutorial](https://github.com/NVIDIA/NeMo/blob/main/tutorials/nlp/Punctuation_and_Capitalization.ipynb) on the topic."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c8cf5e4-49f8-49e7-a763-ab8fb58bfab1",
      "metadata": {
        "id": "3c8cf5e4-49f8-49e7-a763-ab8fb58bfab1"
      },
      "source": [
        "## 4. Riva deployment\n",
        "\n",
        "With all the models trained, now it's the time to deploy the Riva service.\n",
        "\n",
        "### BYO Mandarin models\n",
        "\n",
        "Given the final `.nemo` model, here are the steps that need to be done to deploy on Riva:\n",
        "\n",
        "- Download RIVA Quickstart scripts â€“ it provides `nemo2riva`, `servicemaker`, `riva-speech-server` and `riva-speech-client` images\n",
        "\n",
        "- Build `.riva` assets: using `nemo2riva` command in the `servicemaker` container. See examples of build command for different models and for offline and online ASR pipelines in the [Riva build documentation page]().\n",
        "\n",
        "- Build `RMIR` assets: use the `riva-build` tool in the `servicemaker` container.\n",
        "\n",
        "- Deploy the model and start the server\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96fa5ec0-aed4-4803-94e6-aee34541cf0a",
      "metadata": {
        "id": "96fa5ec0-aed4-4803-94e6-aee34541cf0a"
      },
      "source": [
        "### Riva pretrained Mandarin models on NGC\n",
        "\n",
        "You can use the NGC pretrained Mandarin models as starting points for your development.\n",
        "\n",
        "- [STT Zh Citrinet 1024 Gamma 0.25](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_zh_citrinet_1024_gamma_0_25) Citrinet-1024 model with kernel scaling factor (gamma) of 25%, which has been trained on the open source Aishell-2 Mandarin Chinese corpus. \n",
        "- [RIVA Citrinet ASR Mandarin](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechtotext_zh_cn_citrinet) Citrinet-1024 model which has been trained on the ASR dataset with over 2600 hours of Mandarin(zh-CN) speech. \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30062489-50b0-4c9b-935f-aad180801946",
      "metadata": {
        "id": "30062489-50b0-4c9b-935f-aad180801946"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook, we have guided you through the steps to realize the Riva Mandarin ASR service, from raw data to a ready-to-use service. \n",
        "\n",
        "You can follow the same process to setup a new Mandarin ASR service using your own data, or use the resources in this notebook to fine-tune parts of the pipeline with your own model and data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Mandarin_getting_started.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}